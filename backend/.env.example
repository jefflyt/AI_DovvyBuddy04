ENVIRONMENT=development
DEBUG=true
API_HOST=0.0.0.0
API_PORT=8000
CORS_ORIGINS=http://localhost:3000,http://localhost:3001
DATABASE_URL=postgresql+asyncpg://user:password@localhost:5432/dovvybuddy
DB_POOL_SIZE=10
DB_MAX_OVERFLOW=20

# API Keys
GEMINI_API_KEY=your_gemini_api_key_here
GROQ_API_KEY=your_groq_api_key_here

# LLM Provider Configuration
DEFAULT_LLM_PROVIDER=groq                  # groq | gemini
DEFAULT_LLM_MODEL=gemini-2.0-flash         # Model name (Gemini standard per copilot-instructions.md)
LLM_TEMPERATURE=0.7                        # Generation temperature (0.0-1.0)
LLM_MAX_TOKENS=2048                        # Maximum tokens per generation

# Embedding Configuration
EMBEDDING_MODEL=text-embedding-004         # Gemini embedding model (768 dimensions)
EMBEDDING_BATCH_SIZE=100                   # Max texts per batch request
EMBEDDING_CACHE_SIZE=1000                  # In-memory cache size
EMBEDDING_CACHE_TTL=3600                   # Cache TTL in seconds (1 hour)

# RAG Configuration
ENABLE_RAG=true                            # Enable/disable RAG pipeline
RAG_TOP_K=5                                # Number of chunks to retrieve
RAG_MIN_SIMILARITY=0.5                     # Minimum cosine similarity threshold
RAG_CHUNK_SIZE=512                         # Target chunk size in tokens (not used in chunker yet)
RAG_CHUNK_OVERLAP=50                       # Token overlap between chunks (not used in chunker yet)

# Retry Configuration
LLM_MAX_RETRIES=3                          # Max retry attempts for LLM calls
LLM_RETRY_DELAY=1.0                        # Initial retry delay (seconds)
EMBEDDING_MAX_RETRIES=3                    # Max retry attempts for embeddings
EMBEDDING_RETRY_DELAY=1.0                  # Initial retry delay (seconds)

# Legacy (for reference, not used by Python backend yet)
SESSION_EXPIRY_HOURS=24
MAX_MESSAGE_LENGTH=2000
CONTENT_DIR=../content
