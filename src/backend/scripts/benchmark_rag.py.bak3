"""RAG benchmarking script.

Benchmarks RAG pipeline performance by:
- Running test queries
- Measuring latency (P50, P95, P99)
- Measuring retrieval accuracy (if ground truth provided)
- Outputting results as JSON
"""

import argparse
import asyncio
import asyncio
import json
import statistics
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional

from app.core.config import settings
from app.db.session import SessionLocal
from app.services.rag.pipeline import RAGPipeline
from scripts.common import error, info, progress_bar, success, warning


class BenchmarkResult:
    """Results from a single query benchmark."""
    
    def __init__(self, query: str, ground_truth: Optional[List[str]] = None):
        """Initialize benchmark result.
        
        Args:
            query: Query text
            ground_truth: Expected content paths (for accuracy measurement)
        """
        self.query = query
        self.ground_truth = ground_truth
        self.latency_ms: Optional[float] = None
        self.num_results: int = 0
        self.result_paths: List[str] = []
        self.error: Optional[str] = None
    
    def calculate_accuracy(self) -> Optional[float]:
        """Calculate retrieval accuracy.
        
        Returns:
            Accuracy (0-1) if ground truth available, None otherwise
        """
        if not self.ground_truth or self.error:
            return None
        
        if not self.ground_truth:
            return None
        
        # Calculate how many ground truth items were retrieved
        matches = sum(1 for path in self.ground_truth if path in self.result_paths)
        return matches / len(self.ground_truth) if self.ground_truth else None
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary."""
        result = {
            "query": self.query,
            "latency_ms": self.latency_ms,
            "num_results": self.num_results,
            "result_paths": self.result_paths,
        }
        
        if self.ground_truth:
            result["ground_truth"] = self.ground_truth
            result["accuracy"] = self.calculate_accuracy()
        
        if self.error:
            result["error"] = self.error
        
        return result


def load_queries(queries_file: Path) -> List[Dict[str, Any]]:
    """Load test queries from JSON file.
    
    Args:
        queries_file: Path to queries JSON file
        
    Returns:
        List of query dictionaries
        
    Raises:
        FileNotFoundError: If file not found
        json.JSONDecodeError: If invalid JSON
    """
    with open(queries_file, "r", encoding="utf-8") as f:
        data = json.load(f)
    
    # Support both array and object with "queries" key
    if isinstance(data, list):
        return data
    elif isinstance(data, dict) and "queries" in data:
        return data["queries"]
    else:
        raise ValueError("Invalid queries file format (expected array or object with 'queries' key)")


def run_benchmark_query(
    pipeline: RAGPipeline,
    query: str,
    top_k: int = 5,
) -> BenchmarkResult:
    """Run a single benchmark query.
    
    Args:
        pipeline: RAG pipeline
        query: Query text
        top_k: Number of results to retrieve
        
    Returns:
        BenchmarkResult with timing and results
    """
    result = BenchmarkResult(query)
    
    try:
        start_time = time.perf_counter()
        # Run async retrieve_context in sync context
        rag_context = asyncio.run(pipeline.retrieve_context(query, top_k=top_k))
        end_time = time.perf_counter()
        
        result.latency_ms = (end_time - start_time) * 1000
        result.num_results = len(rag_context.results)
        result.result_paths = [r.metadata.get("content_path", "") for r in rag_context.results]
        
    except Exception as e:
        result.error = str(e)
    
    return result


def calculate_percentile(values: List[float], percentile: int) -> float:
    """Calculate percentile from list of values.
    
    Args:
        values: List of numeric values
        percentile: Percentile to calculate (0-100)
        
    Returns:
        Percentile value
    """
    if not values:
        return 0.0
    
    sorted_values = sorted(values)
    index = (percentile / 100) * (len(sorted_values) - 1)
    
    if index.is_integer():
        return sorted_values[int(index)]
    else:
        lower = sorted_values[int(index)]
        upper = sorted_values[int(index) + 1]
        return lower + (upper - lower) * (index - int(index))


def main():
    """Main entry point for benchmark script."""
    parser = argparse.ArgumentParser(
        description="Benchmark RAG pipeline performance",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Benchmark with default queries file
  python -m scripts.benchmark_rag
  
  # Benchmark with custom queries
  python -m scripts.benchmark_rag --queries-file my_queries.json
  
  # Run 3 iterations per query
  python -m scripts.benchmark_rag --iterations 3
  
  # Save results to custom file
  python -m scripts.benchmark_rag --output results.json
        """,
    )
    parser.add_argument(
        "--queries-file",
        type=Path,
        default=Path("../tests/fixtures/benchmark_queries.json"),
        help="Path to queries JSON file (default: ../tests/fixtures/benchmark_queries.json)",
    )
    parser.add_argument(
        "--output",
        type=Path,
        default=None,
        help="Output JSON file (default: benchmark-results-{timestamp}.json)",
    )
    parser.add_argument(
        "--iterations",
        type=int,
        default=1,
        help="Number of iterations per query (default: 1)",
    )
    parser.add_argument(
        "--top-k",
        type=int,
        default=5,
        help="Number of results to retrieve per query (default: 5)",
    )
    
    args = parser.parse_args()
    
    # Load queries
    try:
        queries_data = load_queries(args.queries_file)
    except FileNotFoundError:
        error(f"Queries file not found: {args.queries_file}")
        sys.exit(1)
    except (json.JSONDecodeError, ValueError) as e:
        error(f"Invalid queries file: {e}")
        sys.exit(1)
    
    if not queries_data:
        warning("No queries found in file")
        sys.exit(0)
    
    info(f"Loaded {len(queries_data)} queries")
    if args.iterations > 1:
        info(f"Running {args.iterations} iteration(s) per query")
    
    # Initialize pipeline (creates default VectorRetriever)
    pipeline = RAGPipeline()
    
    # Run benchmarks
    all_results: List[BenchmarkResult] = []
    all_latencies: List[float] = []
    
    total_runs = len(queries_data) * args.iterations
    
    with progress_bar(total=total_runs, description="Running benchmarks") as bar:
        for query_data in queries_data:
            # Extract query and ground truth
            if isinstance(query_data, str):
                query = query_data
                ground_truth = None
            else:
                query = query_data.get("query", "")
                ground_truth = query_data.get("expected_paths")
            
            if not query:
                warning("Skipping empty query")
                continue
            
            # Run multiple iterations
            iteration_results = []
            for _ in range(args.iterations):
                result = run_benchmark_query(pipeline, query, top_k=args.top_k)
                iteration_results.append(result)
                
                if result.latency_ms:
                    all_latencies.append(result.latency_ms)
                
                bar.update()
            
            # Use median result if multiple iterations
            if args.iterations > 1:
                iteration_results.sort(key=lambda r: r.latency_ms or float('inf'))
                median_result = iteration_results[len(iteration_results) // 2]
            else:
                median_result = iteration_results[0]
            
            median_result.ground_truth = ground_truth
            all_results.append(median_result)
        
        # Calculate statistics
        print()
        success(f"Completed {total_runs} benchmark run(s)")
        print()
        
        if all_latencies:
            info("Latency Statistics:")
            info(f"  Mean: {statistics.mean(all_latencies):.2f}ms")
            info(f"  Median (P50): {calculate_percentile(all_latencies, 50):.2f}ms")
            info(f"  P95: {calculate_percentile(all_latencies, 95):.2f}ms")
            info(f"  P99: {calculate_percentile(all_latencies, 99):.2f}ms")
            info(f"  Min: {min(all_latencies):.2f}ms")
            info(f"  Max: {max(all_latencies):.2f}ms")
        
        # Calculate accuracy if ground truth available
        accuracy_results = [r.calculate_accuracy() for r in all_results if r.calculate_accuracy() is not None]
        if accuracy_results:
            print()
            info("Accuracy Statistics:")
            info(f"  Mean: {statistics.mean(accuracy_results):.2%}")
            info(f"  Median: {statistics.median(accuracy_results):.2%}")
            info(f"  Min: {min(accuracy_results):.2%}")
            info(f"  Max: {max(accuracy_results):.2%}")
        
        # Count errors
        error_count = sum(1 for r in all_results if r.error)
        if error_count > 0:
            print()
            warning(f"Errors: {error_count}/{len(all_results)} queries failed")
        
        # Prepare output
        output_data = {
            "timestamp": datetime.now().isoformat(),
            "queries_file": str(args.queries_file),
            "iterations": args.iterations,
            "top_k": args.top_k,
            "total_queries": len(queries_data),
            "total_runs": total_runs,
            "statistics": {
                "latency": {
                    "mean_ms": statistics.mean(all_latencies) if all_latencies else None,
                    "median_ms": calculate_percentile(all_latencies, 50) if all_latencies else None,
                    "p95_ms": calculate_percentile(all_latencies, 95) if all_latencies else None,
                    "p99_ms": calculate_percentile(all_latencies, 99) if all_latencies else None,
                    "min_ms": min(all_latencies) if all_latencies else None,
                    "max_ms": max(all_latencies) if all_latencies else None,
                },
            },
            "results": [r.to_dict() for r in all_results],
        }
        
        if accuracy_results:
            output_data["statistics"]["accuracy"] = {
                "mean": statistics.mean(accuracy_results),
                "median": statistics.median(accuracy_results),
                "min": min(accuracy_results),
                "max": max(accuracy_results),
            }
        
        # Write output file
        if args.output:
            output_file = args.output
        else:
            timestamp = datetime.now().strftime("%Y-%m-%dT%H-%M-%S")
            output_file = Path(f"benchmark-results-{timestamp}.json")
        
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(output_data, f, indent=2)
        
        success(f"Results written to: {output_file}")


if __name__ == "__main__":
    main()
